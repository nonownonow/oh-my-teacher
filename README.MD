# 최종 결과물 가이드: 나의 과외 선생님 (OhMyTeacher)
OhMyTeacher
## 1. 문제 정의 & 목표

- **서비스 한 줄 요약**: PDF 교재/문서 기반으로 텍스트와 이미지를 모두 이해하여 질문에 답해주는 AI 과외 선생님 서비스
- **해결하려는 문제 및 중요성**:
  - 범용 LLM(ChatGPT 등)은 일반적인 지식은 풍부하지만, 사용자가 보유한 특정 전공 서적이나 실무 문서에 대한 깊이 있는 답변은 어렵습니다.
  - 특히 표, 차트, 다이어그램이 많은 전문 서적의 경우 텍스트만으로는 이해가 불완전합니다.
  - 사용자가 보유한 양질의 데이터를 토대로 정확하고 맥락에 맞는 답변을 제공하여 학습/업무 효율을 극대화하는 것이 목표입니다.

## 2. 서비스 구조 & 데이터 흐름

- **전체 서비스 구조**:

  - **Input**: 사용자 PDF 파일 업로드 및 OpenAI API Key 입력
  - **Process**:
    1. `pdfplumber`를 이용해 페이지별 텍스트 및 이미지 추출
    2. 텍스트는 `RecursiveCharacterTextSplitter`로 청크 분할
    3. 이미지는 로컬 캐시 디렉토리에 저장 및 메타데이터 연결
  - **AI/LLM**:
    1. 텍스트 임베딩 (`text-embedding-3-large`) 생성 후 ChromaDB 저장
    2. 사용자 질문 시 `MultiQueryRetriever`로 관련 텍스트 검색
    3. 검색된 텍스트와 연관된 이미지를 함께 프롬프트에 포함
    4. 멀티모달 LLM (`gpt-4o-mini`)이 최종 답변 생성
  - **Result**: Streamlit UI를 통해 스트리밍 형태로 답변 및 채팅 제공

- **주요 기능**:
  - **멀티모달 RAG**: 텍스트뿐만 아니라 PDF 내의 시각 자료(이미지)를 함께 분석.
  - **스트리밍 답변**: `StreamHandler`를 통해 실시간으로 답변 생성 과정 표시.
  - **문서 기반 검색**: 업로드한 문서 범위 내에서 할루시네이션을 줄이고 사실 기반 답변 유도.

## 3. AI/LLM 활용

- **AI/LLM 활용 시나리오**:
  - 학생이 전공 서적을 업로드하고, "이 도표가 설명하는 것이 뭐야?"라고 질문하면, 관련 페이지의 텍스트와 도표 이미지를 함께 분석해 설명해줌.
  - 실무자가 매뉴얼을 업로드하고, 특정 절차에 대해 질문하면 정확한 단계를 안내해줌.
- **AI/LLM 아키텍처**:
  - **사용 모델**: `gpt-4o-mini` (텍스트와 이미지를 동시에 처리할 수 있는 멀티모달 능력과 비용 효율성 고려)
  - **임베딩 모델**: `text-embedding-3-large` (한국어 및 전문 용어 이해도가 높음)
  - **구성 요소**:
    - **LangChain**: RAG 파이프라인 구성 및 프롬프트 관리
    - **ChromaDB**: 벡터 데이터베이스 (유사도 검색)
    - **MultiQueryRetriever**: 질문을 다양한 각도로 변형하여 검색 정확도(Recall) 향상
- **성과 지표**:
  - **품질 지표**: 시각 자료(그래프, 장표)가 포함된 질문에 대한 답변 정확도 대폭 향상 (기존 텍스트 전용 대비)
  - **운영 지표**: `gpt-4o-mini` 사용으로 응답 속도(Latency) 최적화 및 비용 절감

## 4. 모델 개발 & 적용 (변경 사항)

- **Baseline 대비 변화**:
  - 기존: 텍스트만 추출하여 일반 텍스트기반 RAG 수행 -> 도표나 그래프 질문에 답변 불가.
  - **개선**: PDF 페이지를 이미지로 렌더링하고, 검색된 맥락(Context)에 해당 이미지를 멀티모달 LLM에 함께 입력값으로 전달하도록 구조 변경.
  - 기존 로컬 Ollama(Llama 3등) 시도에서 안정적인 배포와 멀티모달 성능을 위해 OpenAI 스택으로 통합 및 최적화.

## 5. 결과 & 분석

- **구현 결과 요약**:
  - Streamlit을 통해 직관적인 챗봇 형태의 UI 구현 완료.
  - PDF 업로드 즉시 벡터 DB 구축 및 대화 시작 가능.
- **AI/LLM 적용 효과**:
  - 단순 키워드 매칭을 넘어 의미 기반 검색과 시각적 이해가 결합된 고차원적인 질의응답 가능.
- **출력/응답 예시**:
  - 질문: "3페이지의 구조도에 대해 설명해줘"
  - 답변: (3페이지 텍스트와 3페이지 이미지를 보고) "네, 해당 구조도는 시스템의 모듈 간 통신 방식을 나타냅니다. A모듈은 B모듈로..."

## 6. 한계 & 향후 개선

- **현재 한계**:
  - OpenAI API 의존성으로 인한 사용 비용 발생.
  - 이미지 처리로 인한 토큰 사용량 증가 가능성.
  - 배포 환경(Streamlit Cloud)에서의 리소스 제약(메모리 등).
- **향후 고도화 방향**:
  - **로컬 LLM 지원**: 비용 절감 및 보안 강화를 위해 Ollama(Llama 3.2 Vision 등) 연동 재도입 검토.
  - **다양한 문서 지원**: PDF 외에 PPT, DOCX 등 지원 포맷 확장.
  - **하이브리드 검색**: 키워드 검색(BM25)과 벡터 검색(Dense)을 결합하여 검색 정확도 추가 향상.
