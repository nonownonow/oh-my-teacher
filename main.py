# Streamlit Cloud ë°°í¬ìš© (Linux í™˜ê²½)
try:
    __import__('pysqlite3')
    import sys
    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
except ImportError:
    pass

from langchain_core.documents import Document
from langchain_core.messages import HumanMessage, SystemMessage
import pdfplumber
import os
import streamlit as st
import tempfile
import chromadb
from dotenv import load_dotenv
from langchain.callbacks.base import BaseCallbackHandler
from streamlit_extras.buy_me_a_coffee import button
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_ollama import ChatOllama
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter

# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ì½ê¸° (Ollamaë§Œ)
OLLAMA_API_KEY = os.getenv("OLLAMA_AI_API", "")

# ì œëª© ë° ìŠ¤íƒ€ì¼
st.set_page_config(page_title="ë‚˜ì˜ ê³¼ì™¸ ì„ ìƒë‹˜ ğŸ‘¨â€ğŸ«", page_icon="ğŸ‘¨â€ğŸ«")
st.title("ë‚˜ì˜ ê³¼ì™¸ ì„ ìƒë‹˜")
st.markdown("""
<style>
    .reportview-container {
        background: #f0f2f6
    }
</style>
""", unsafe_allow_html=True)
st.write("---")

# --------------------------------------------------------------------------------
# Sidebar
# --------------------------------------------------------------------------------
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    # Model Selection
    model_provider = st.radio(
        "ëª¨ë¸ ì„ íƒ",
        ["GPT-4o (ìƒìš©/ê³ í’ˆì§ˆ)", "Ollama (ì„¤ì¹˜í˜•/ë³´ì•ˆ)", "í•˜ì´ë¸Œë¦¬ë“œ (GPTë¶„ì„+Ollamaë‹µë³€)"],
        index=2,
        help="í•˜ì´ë¸Œë¦¬ë“œ: GPTì—ê²Œ ì§ˆë¬¸ë§Œ ì „ì†¡í•˜ì—¬ ì¶”ë¡  í”„ë ˆì„ì›Œí¬ë¥¼ ë°›ê³ , Ollamaê°€ PDF ë‚´ìš©ê³¼ ê²°í•©í•˜ì—¬ ë‹µë³€ ìƒì„± (PDF ë³´ì•ˆ ìœ ì§€)"
    )

    ollama_url = "https://ollama.com"
    ollama_key = OLLAMA_API_KEY

    # GPT/í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œì¼ ë•Œ OpenAI í‚¤ ì™¸ë¶€ ì…ë ¥ (í•­ìƒ)
    openai_key = ""
    if model_provider in ["GPT-4o (ìƒìš©/ê³ í’ˆì§ˆ)", "í•˜ì´ë¸Œë¦¬ë“œ (GPTë¶„ì„+Ollamaë‹µë³€)"]:
        openai_key = st.text_input('OpenAI API Key', type="password", help="GPT ëª¨ë¸ ì‚¬ìš©ì„ ìœ„í•œ API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”")

    st.divider()
    button(username="{ê³„ì • ID}", floating=False, width=221)

# File Upload
uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì˜¬ë ¤ì£¼ì„¸ìš”!", type=['pdf'])
st.write("---")

# --------------------------------------------------------------------------------
# Logic
# --------------------------------------------------------------------------------


class StreamHandler(BaseCallbackHandler):
    def __init__(self, container, initial_text=""):
        self.container = container
        self.text = initial_text

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text)


def get_reasoning_framework_from_gpt(question: str, api_key: str) -> str:
    """GPTë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•œ ì¶”ë¡  í”„ë ˆì„ì›Œí¬/ê°€ì´ë“œë¥¼ ìƒì„± (PDF ë‚´ìš© ì—†ì´ ì§ˆë¬¸ë§Œ ì „ì†¡)"""
    llm = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0,
        openai_api_key=api_key,
    )

    # PDF ë‚´ìš© ì—†ì´ ì§ˆë¬¸ë§Œ GPTì—ê²Œ ì „ì†¡
    framework_prompt = f"""ë‹¹ì‹ ì€ ì§ˆë¬¸ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸° ìœ„í•œ ì²´ê³„ì ì¸ ë¶„ì„ í”„ë ˆì„ì›Œí¬ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.

[ì§ˆë¬¸]
{question}

[ì§€ì‹œì‚¬í•­]
1. ì´ ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸° ìœ„í•´ ë¬¸ì„œì—ì„œ ì°¾ì•„ì•¼ í•  í•µì‹¬ ìš”ì†Œë“¤ì„ ë‚˜ì—´í•˜ì„¸ìš”.
2. ë‹µë³€ì„ êµ¬ì„±í•  ë•Œ ê³ ë ¤í•´ì•¼ í•  ë…¼ë¦¬ì  ë‹¨ê³„ë¥¼ ì œì‹œí•˜ì„¸ìš”.
3. ì¢‹ì€ ë‹µë³€ì˜ êµ¬ì¡°ì™€ í¬í•¨í•´ì•¼ í•  ë‚´ìš©ì„ ì•ˆë‚´í•˜ì„¸ìš”.
4. ë‹µë³€ ì‹œ ì£¼ì˜í•´ì•¼ í•  ì ì´ë‚˜ í”í•œ ì‹¤ìˆ˜ë¥¼ ì–¸ê¸‰í•˜ì„¸ìš”.
5. í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.

[ë¶„ì„ í”„ë ˆì„ì›Œí¬]"""

    response = llm.invoke([HumanMessage(content=framework_prompt)])
    return response.content


@st.cache_resource(show_spinner="ë¬¸ì„œ ë¶„ì„ ë° ì„ë² ë”© ì¤‘...")
def embed_file(file, provider, _api_key):
    file_content = file.read()

    # Use a temporary directory for file storage to avoid clutter
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(file_content)
        tmp_file_path = tmp_file.name

    documents = []
    # PDF Parsing with pdfplumber (Text only)
    with pdfplumber.open(tmp_file_path) as pdf:
        for i, page in enumerate(pdf.pages):
            text = page.extract_text() or ""
            documents.append(Document(page_content=text, metadata={
                             "page": i+1, "source": file.name}))

    # Clean up temp file
    os.unlink(tmp_file_path)

    # Text Splitter
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, chunk_overlap=100)
    texts = text_splitter.split_documents(documents)

    # Embedding Logic - GPT ë˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œëŠ” OpenAI ì„ë² ë”© ì‚¬ìš©
    if provider in ["GPT-4o (ìƒìš©/ê³ í’ˆì§ˆ)", "í•˜ì´ë¸Œë¦¬ë“œ (GPTë¶„ì„+Ollamaë‹µë³€)"]:
        if not _api_key:
            st.error("OpenAI API Key Required")
            st.stop()
        embeddings_model = OpenAIEmbeddings(
            model="text-embedding-3-small", openai_api_key=_api_key)
        collection_name = "openai_collection"
    else:
        # HuggingFace Embeddings (ë¬´ë£Œ, ë¡œì»¬ ì‹¤í–‰)
        embeddings_model = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )
        collection_name = "hf_collection"

    # Chroma DB - Persistent Client
    safe_name = "".join([c for c in file.name if c.isalnum()])
    provider_prefix = "gpt" if provider != "Ollama (ì„¤ì¹˜í˜•/ë³´ì•ˆ)" else "ollama"
    persist_dir = os.path.join(
        os.getcwd(), ".chroma_db", provider_prefix, safe_name)

    client = chromadb.PersistentClient(path=persist_dir)

    db = Chroma.from_documents(
        texts,
        embeddings_model,
        client=client,
        collection_name=collection_name
    )

    return db.as_retriever(search_kwargs={"k": 5})


if uploaded_file is not None:
    try:
        retriever = embed_file(uploaded_file, model_provider, openai_key)
    except Exception as e:
        st.error(f"Error: {e}")
        st.stop()

    # Session State
    if "messages" not in st.session_state:
        st.session_state["messages"] = [
            {"role": "assistant", "content": "ë„¤, ë¬¸ì„œë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸í•´ ì£¼ì„¸ìš”."}]

    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).write(msg["content"])

    # Chat Input
    if prompt_message := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
        st.chat_message("user").write(prompt_message)
        st.session_state.messages.append(
            {"role": "user", "content": prompt_message})

        with st.chat_message("assistant"):
            status_container = st.empty()

            # Retrieval
            docs = retriever.invoke(prompt_message)
            context_text = "\n\n".join(doc.page_content for doc in docs)

            # Generation
            if model_provider == "GPT-4o (ìƒìš©/ê³ í’ˆì§ˆ)":
                llm = ChatOpenAI(
                    model="gpt-4o",
                    temperature=0,
                    openai_api_key=openai_key,
                    streaming=True,
                    callbacks=[StreamHandler(status_container)]
                )

                system_prompt = (
                    "ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ ë‹µë³€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
                    "ì•„ë˜ ì œê³µëœ ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ìƒì„¸í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n\n"
                    f"[ë¬¸ë§¥]\n{context_text}\n\n"
                    "[ì§€ì‹œì‚¬í•­]\n"
                    "- ë¬¸ë§¥ì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì•„ êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n"
                    "- í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”."
                )

                response = llm.invoke([
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=prompt_message)
                ])

            elif model_provider == "í•˜ì´ë¸Œë¦¬ë“œ (GPTë¶„ì„+Ollamaë‹µë³€)":
                # 1ë‹¨ê³„: GPTë¡œ ì¶”ë¡  í”„ë ˆì„ì›Œí¬ ìƒì„± (PDF ë‚´ìš© ì—†ì´ ì§ˆë¬¸ë§Œ ì „ì†¡)
                status_container.markdown("ğŸ§  GPTê°€ ì¶”ë¡  í”„ë ˆì„ì›Œí¬ë¥¼ ìƒì„± ì¤‘... (PDF ë‚´ìš©ì€ ì „ì†¡ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤)")

                reasoning_framework = get_reasoning_framework_from_gpt(
                    prompt_message, openai_key
                )

                # 2ë‹¨ê³„: Ollamaê°€ PDF ë‚´ìš© + GPT ì¶”ë¡  í”„ë ˆì„ì›Œí¬ë¥¼ ê²°í•©í•˜ì—¬ ë‹µë³€ ìƒì„±
                status_container.markdown("âœï¸ Ollamaê°€ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ê³  ë‹µë³€ì„ ìƒì„± ì¤‘...")

                headers = {}
                if ollama_key:
                    headers["Authorization"] = f"Bearer {ollama_key}"

                llm = ChatOllama(
                    base_url=ollama_url,
                    model="gemma3:27b",
                    temperature=0,
                    streaming=True,
                    callbacks=[StreamHandler(status_container)],
                    client_kwargs={"headers": headers} if headers else {}
                )

                system_prompt = f"""ë‹¹ì‹ ì€ ì¹œì ˆí•œ ê³¼ì™¸ ì„ ìƒë‹˜ì…ë‹ˆë‹¤.
ì•„ë˜ ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ê³¼ ë¶„ì„ í”„ë ˆì„ì›Œí¬ë¥¼ í™œìš©í•˜ì—¬ í•™ìƒì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

[ë¬¸ì„œ ë‚´ìš©]
{context_text}

[GPTê°€ ì œê³µí•œ ë¶„ì„ í”„ë ˆì„ì›Œí¬]
{reasoning_framework}

[ì§€ì‹œì‚¬í•­]
- ìœ„ ë¶„ì„ í”„ë ˆì„ì›Œí¬ì˜ ê°€ì´ë“œë¥¼ ë”°ë¼ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ìœ¼ì„¸ìš”.
- í”„ë ˆì„ì›Œí¬ì—ì„œ ì œì‹œí•œ ë…¼ë¦¬ì  ë‹¨ê³„ì— ë”°ë¼ ë‹µë³€ì„ êµ¬ì„±í•˜ì„¸ìš”.
- ë¬¸ì„œì˜ êµ¬ì²´ì ì¸ ë‚´ìš©ì„ ì¸ìš©í•˜ë©° ë‹µë³€í•˜ì„¸ìš”.
- í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”."""

                response = llm.invoke([
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=prompt_message)
                ])

            else:  # Ollama (ì„¤ì¹˜í˜•/ë³´ì•ˆ)
                headers = {}
                if ollama_key:
                    headers["Authorization"] = f"Bearer {ollama_key}"

                llm = ChatOllama(
                    base_url=ollama_url,
                    model="gemma3:27b",
                    temperature=0,
                    streaming=True,
                    callbacks=[StreamHandler(status_container)],
                    client_kwargs={"headers": headers} if headers else {}
                )

                system_prompt = (
                    "ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ ë‹µë³€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
                    "ì•„ë˜ ì œê³µëœ ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ìƒì„¸í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n\n"
                    f"[ë¬¸ë§¥]\n{context_text}\n\n"
                    "[ì§€ì‹œì‚¬í•­]\n"
                    "- ë¬¸ë§¥ì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì•„ êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n"
                    "- í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”."
                )

                response = llm.invoke([
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=prompt_message)
                ])

            response_content = response.content

        st.session_state.messages.append(
            {"role": "assistant", "content": response_content})
