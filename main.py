# Streamlit Cloud ë°°í¬ìš© (Linux í™˜ê²½)
try:
    __import__('pysqlite3')
    import sys
    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
except ImportError:
    pass

from langchain_core.documents import Document
from langchain_core.messages import HumanMessage, SystemMessage
import pdfplumber
import os
import streamlit as st
import tempfile
import chromadb
from dotenv import load_dotenv
from langchain.callbacks.base import BaseCallbackHandler
from streamlit_extras.buy_me_a_coffee import button
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_ollama import ChatOllama
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter

# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ì½ê¸° (Ollamaë§Œ)
OLLAMA_API_KEY = os.getenv("OLLAMA_AI_API", "")

# ì œëª© ë° ìŠ¤íƒ€ì¼
st.set_page_config(page_title="ë‚˜ì˜ ê³¼ì™¸ ì„ ìƒë‹˜ ğŸ‘¨â€ğŸ«", page_icon="ğŸ‘¨â€ğŸ«")
st.title("ë‚˜ì˜ ê³¼ì™¸ ì„ ìƒë‹˜")
st.markdown("""
<style>
    .reportview-container {
        background: #f0f2f6
    }
</style>
""", unsafe_allow_html=True)
st.write("---")

# --------------------------------------------------------------------------------
# Sidebar
# --------------------------------------------------------------------------------
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    # Model Selection
    model_provider = st.radio(
        "ëª¨ë¸ ì„ íƒ",
        ["GPT-4o (ìƒìš©/ê³ í’ˆì§ˆ)", "Ollama (ì„¤ì¹˜í˜•/ë³´ì•ˆ)", "í•˜ì´ë¸Œë¦¬ë“œ (GPTë²¡í„°ì¶”ë¡ +Ollamaë‹µë³€)"],
        index=2,
        help="í•˜ì´ë¸Œë¦¬ë“œ: GPTê°€ ì§ˆë¬¸ë§Œ ë°›ì•„ ë²¡í„° ì¶”ë¡ (ì˜ë¯¸ í™•ì¥) ìˆ˜í–‰ â†’ Ollamaê°€ í–¥ìƒëœ ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ê³ í’ˆì§ˆ ë‹µë³€ ìƒì„± (PDF ë‚´ìš© ë³´í˜¸)"
    )

    ollama_url = "https://ollama.com"
    ollama_key = OLLAMA_API_KEY

    # GPT/í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œì¼ ë•Œ OpenAI í‚¤ ì™¸ë¶€ ì…ë ¥ (í•­ìƒ)
    openai_key = ""
    if model_provider in ["GPT-4o (ìƒìš©/ê³ í’ˆì§ˆ)", "í•˜ì´ë¸Œë¦¬ë“œ (GPTë²¡í„°ì¶”ë¡ +Ollamaë‹µë³€)"]:
        openai_key = st.text_input('OpenAI API Key', type="password", help="GPT ëª¨ë¸ ì‚¬ìš©ì„ ìœ„í•œ API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”")

    st.divider()
    button(username="{ê³„ì • ID}", floating=False, width=221)

# File Upload
uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì˜¬ë ¤ì£¼ì„¸ìš”!", type=['pdf'])
st.write("---")

# --------------------------------------------------------------------------------
# Logic
# --------------------------------------------------------------------------------


class StreamHandler(BaseCallbackHandler):
    def __init__(self, container, initial_text=""):
        self.container = container
        self.text = initial_text

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text)


def get_semantic_expansion_from_gpt(question: str, api_key: str) -> dict:
    """
    GPTë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì˜ ì˜ë¯¸ì  í™•ì¥ì„ ìˆ˜í–‰ (ë²¡í„° ì¶”ë¡ )
    PDF ë‚´ìš© ì—†ì´ ì§ˆë¬¸ë§Œ ì „ì†¡í•˜ì—¬ ê´€ë ¨ ê°œë…, ë™ì˜ì–´, í•˜ìœ„ ì§ˆë¬¸ì„ ìƒì„±
    ì´ë¥¼ í†µí•´ ë²¡í„° ê²€ìƒ‰ì˜ í’ˆì§ˆì„ í–¥ìƒì‹œí‚´
    """
    llm = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0.3,  # ì•½ê°„ì˜ ì°½ì˜ì„± í—ˆìš©
        openai_api_key=api_key,
    )

    # GPTì—ê²Œ ì§ˆë¬¸ì˜ ì˜ë¯¸ì  í™•ì¥ ìš”ì²­ (ë²¡í„° ê³µê°„ì—ì„œì˜ ê´€ê³„ ì¶”ë¡ )
    expansion_prompt = f"""ë‹¹ì‹ ì€ ì˜ë¯¸ë¡ ì  ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ë²¡í„° ê²€ìƒ‰ í’ˆì§ˆì„ ë†’ì´ê¸° ìœ„í•œ ì˜ë¯¸ì  í™•ì¥ì„ ìˆ˜í–‰í•˜ì„¸ìš”.

[ì›ë³¸ ì§ˆë¬¸]
{question}

[ì§€ì‹œì‚¬í•­]
JSON í˜•ì‹ìœ¼ë¡œ ë‹¤ìŒì„ ì œê³µí•˜ì„¸ìš”:
1. "core_concepts": ì§ˆë¬¸ì˜ í•µì‹¬ ê°œë… í‚¤ì›Œë“œ (3-5ê°œ)
2. "synonyms": ê° í•µì‹¬ ê°œë…ì˜ ë™ì˜ì–´/ìœ ì‚¬ì–´ (ê°œë…ë‹¹ 2-3ê°œ)
3. "sub_questions": ì›ë³¸ ì§ˆë¬¸ì„ ë‹µí•˜ê¸° ìœ„í•´ í•„ìš”í•œ í•˜ìœ„ ì§ˆë¬¸ë“¤ (3-5ê°œ)
4. "related_topics": ê´€ë ¨ë  ìˆ˜ ìˆëŠ” ì£¼ì œ/ë§¥ë½ (3-5ê°œ)
5. "search_queries": ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•  ìµœì í™”ëœ ì¿¼ë¦¬ë¬¸ (3-5ê°œ)

[ì¶œë ¥ í˜•ì‹]
```json
{{
  "core_concepts": ["ê°œë…1", "ê°œë…2", ...],
  "synonyms": {{"ê°œë…1": ["ë™ì˜ì–´1", "ë™ì˜ì–´2"], ...}},
  "sub_questions": ["í•˜ìœ„ì§ˆë¬¸1", "í•˜ìœ„ì§ˆë¬¸2", ...],
  "related_topics": ["ì£¼ì œ1", "ì£¼ì œ2", ...],
  "search_queries": ["ì¿¼ë¦¬1", "ì¿¼ë¦¬2", ...]
}}
```"""

    response = llm.invoke([HumanMessage(content=expansion_prompt)])

    # JSON íŒŒì‹± ì‹œë„
    import json
    import re
    try:
        # JSON ë¸”ë¡ ì¶”ì¶œ
        json_match = re.search(r'```json\s*(.*?)\s*```', response.content, re.DOTALL)
        if json_match:
            return json.loads(json_match.group(1))
        else:
            # JSON ë¸”ë¡ ì—†ì´ ì§ì ‘ íŒŒì‹± ì‹œë„
            return json.loads(response.content)
    except json.JSONDecodeError:
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
        return {
            "core_concepts": [question],
            "synonyms": {},
            "sub_questions": [question],
            "related_topics": [],
            "search_queries": [question]
        }


def enhanced_vector_search(retriever, question: str, semantic_expansion: dict, k: int = 5) -> list:
    """
    GPTì˜ ì˜ë¯¸ì  í™•ì¥ì„ í™œìš©í•œ í–¥ìƒëœ ë²¡í„° ê²€ìƒ‰
    ì—¬ëŸ¬ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ í›„ ì¤‘ë³µ ì œê±° ë° ê²°ê³¼ ë³‘í•©
    """
    all_docs = []
    seen_contents = set()

    # 1. ì›ë³¸ ì§ˆë¬¸ìœ¼ë¡œ ê²€ìƒ‰
    original_docs = retriever.invoke(question)
    for doc in original_docs:
        if doc.page_content not in seen_contents:
            seen_contents.add(doc.page_content)
            all_docs.append(doc)

    # 2. í™•ì¥ëœ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ì¶”ê°€ ê²€ìƒ‰
    search_queries = semantic_expansion.get("search_queries", [])
    for query in search_queries[:3]:  # ìµœëŒ€ 3ê°œ ì¿¼ë¦¬
        try:
            docs = retriever.invoke(query)
            for doc in docs:
                if doc.page_content not in seen_contents:
                    seen_contents.add(doc.page_content)
                    all_docs.append(doc)
        except Exception:
            continue

    # 3. í•˜ìœ„ ì§ˆë¬¸ìœ¼ë¡œ ì¶”ê°€ ê²€ìƒ‰
    sub_questions = semantic_expansion.get("sub_questions", [])
    for sub_q in sub_questions[:2]:  # ìµœëŒ€ 2ê°œ í•˜ìœ„ ì§ˆë¬¸
        try:
            docs = retriever.invoke(sub_q)
            for doc in docs:
                if doc.page_content not in seen_contents:
                    seen_contents.add(doc.page_content)
                    all_docs.append(doc)
        except Exception:
            continue

    # ê²°ê³¼ ê°œìˆ˜ ì œí•œ
    return all_docs[:k]


def map_reduce_with_ollama(
    docs: list,
    question: str,
    semantic_expansion: dict,
    ollama_url: str,
    ollama_key: str,
    status_container,
    batch_size: int = 2
) -> str:
    """
    Map-Reduce íŒ¨í„´ìœ¼ë¡œ ë¬¸ì„œë¥¼ ë¶„í•  ì²˜ë¦¬ í›„ í•©ì¹¨
    1. Map: ê° ë¬¸ì„œ ë°°ì¹˜ì—ì„œ ê´€ë ¨ ì •ë³´ ì¶”ì¶œ
    2. Reduce: ì¶”ì¶œëœ ì •ë³´ë“¤ì„ í•©ì³ì„œ ìµœì¢… ë‹µë³€ ìƒì„±
    """
    headers = {}
    if ollama_key:
        headers["Authorization"] = f"Bearer {ollama_key}"

    # Map ë‹¨ê³„ìš© LLM (ìŠ¤íŠ¸ë¦¬ë° ì—†ì´)
    map_llm = ChatOllama(
        base_url=ollama_url,
        model="gemma3:27b",
        temperature=0,
        streaming=False,
        client_kwargs={"headers": headers} if headers else {}
    )

    # ë¬¸ì„œë¥¼ ë°°ì¹˜ë¡œ ë¶„í• 
    batches = [docs[i:i + batch_size] for i in range(0, len(docs), batch_size)]

    # Map ë‹¨ê³„: ê° ë°°ì¹˜ì—ì„œ ê´€ë ¨ ì •ë³´ ì¶”ì¶œ
    extracted_infos = []
    for idx, batch in enumerate(batches):
        status_container.markdown(f"ğŸ“„ ë¬¸ì„œ ë¶„ì„ ì¤‘... ({idx + 1}/{len(batches)})")

        batch_content = "\n\n".join(doc.page_content for doc in batch)

        map_prompt = f"""ë‹¤ìŒ ë¬¸ì„œì—ì„œ ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸° ìœ„í•´ í•„ìš”í•œ í•µì‹¬ ì •ë³´ë§Œ ì¶”ì¶œí•˜ì„¸ìš”.

[ë¬¸ì„œ]
{batch_content}

[ì§ˆë¬¸]
{question}

[í•µì‹¬ ê°œë… ì°¸ê³ ]
{', '.join(semantic_expansion.get('core_concepts', []))}

[ì§€ì‹œì‚¬í•­]
- ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ë§Œ ê°„ê²°í•˜ê²Œ ì¶”ì¶œí•˜ì„¸ìš”.
- ë¶ˆí•„ìš”í•œ ì •ë³´ëŠ” ì œì™¸í•˜ì„¸ìš”.
- ê´€ë ¨ ì •ë³´ê°€ ì—†ìœ¼ë©´ "ê´€ë ¨ ì •ë³´ ì—†ìŒ"ì´ë¼ê³  ë‹µí•˜ì„¸ìš”.
- êµìœ¡ì ìœ¼ë¡œ ë¶€ì ì ˆí•œ í‘œí˜„ì€ ìˆœí™”ëœ í‘œí˜„ìœ¼ë¡œ ëŒ€ì²´í•˜ì„¸ìš”.

[ì¶”ì¶œëœ ì •ë³´]"""

        try:
            response = map_llm.invoke([HumanMessage(content=map_prompt)])
            if "ê´€ë ¨ ì •ë³´ ì—†ìŒ" not in response.content:
                extracted_infos.append(response.content)
        except Exception:
            continue

    if not extracted_infos:
        return "ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # Reduce ë‹¨ê³„: ì¶”ì¶œëœ ì •ë³´ë“¤ì„ í•©ì³ì„œ ìµœì¢… ë‹µë³€ ìƒì„±
    status_container.markdown("âœï¸ ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘...")

    combined_info = "\n\n---\n\n".join(extracted_infos)

    # Reduceìš© LLM (ìŠ¤íŠ¸ë¦¬ë° í¬í•¨)
    reduce_llm = ChatOllama(
        base_url=ollama_url,
        model="gemma3:27b",
        temperature=0,
        streaming=True,
        callbacks=[StreamHandler(status_container)],
        client_kwargs={"headers": headers} if headers else {}
    )

    expansion_info = f"""[GPT ë²¡í„° ì¶”ë¡  ê²°ê³¼]
- í•µì‹¬ ê°œë…: {', '.join(semantic_expansion.get('core_concepts', []))}
- ê´€ë ¨ ì£¼ì œ: {', '.join(semantic_expansion.get('related_topics', []))}
- ë¶„ì„ ê´€ì : {', '.join(semantic_expansion.get('sub_questions', [])[:3])}"""

    reduce_prompt = f"""ë‹¹ì‹ ì€ ì¹œì ˆí•œ ê³¼ì™¸ ì„ ìƒë‹˜ì…ë‹ˆë‹¤.
ì•„ë˜ ë¬¸ì„œì—ì„œ ì¶”ì¶œëœ ì •ë³´ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ í•™ìƒì˜ ì§ˆë¬¸ì— ìƒì„¸íˆ ë‹µë³€í•˜ì„¸ìš”.

[ì¶”ì¶œëœ í•µì‹¬ ì •ë³´ë“¤]
{combined_info}

{expansion_info}

[ì§€ì‹œì‚¬í•­]
- ì¶”ì¶œëœ ì •ë³´ë“¤ì„ ì¢…í•©í•˜ì—¬ ì™„ì„±ë„ ë†’ì€ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.
- êµ¬ì²´ì ì¸ ë‚´ìš©ì„ ì¸ìš©í•˜ë©° ì„¤ëª…í•˜ì„¸ìš”.
- í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

[ì½˜í…ì¸  í•„í„°ë§ - í•„ìˆ˜]
- êµìœ¡ì ìœ¼ë¡œ ë¶€ì ì ˆí•œ í‘œí˜„(ë‚œë´‰, ë°”ëŒë‘¥ì´, ìƒ‰ê³¨ ë“±)ì€ ìˆœí™”ëœ í‘œí˜„ìœ¼ë¡œ ëŒ€ì²´í•˜ì„¸ìš”.
- ì„ ì •ì ì´ê±°ë‚˜ í­ë ¥ì ì¸ ë¬˜ì‚¬ëŠ” í”¼í•˜ê³  êµìœ¡ì ìœ¼ë¡œ ì í•©í•œ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.
- í•™ìƒì—ê²Œ ì í•©í•œ í’ˆìœ„ ìˆëŠ” ì–¸ì–´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."""

    response = reduce_llm.invoke([
        SystemMessage(content=reduce_prompt),
        HumanMessage(content=question)
    ])

    return response.content


@st.cache_resource(show_spinner="ë¬¸ì„œ ë¶„ì„ ë° ì„ë² ë”© ì¤‘...")
def embed_file(file, provider, _api_key):
    file_content = file.read()

    # Use a temporary directory for file storage to avoid clutter
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(file_content)
        tmp_file_path = tmp_file.name

    documents = []
    # PDF Parsing with pdfplumber (Text only)
    with pdfplumber.open(tmp_file_path) as pdf:
        for i, page in enumerate(pdf.pages):
            text = page.extract_text() or ""
            documents.append(Document(page_content=text, metadata={
                             "page": i+1, "source": file.name}))

    # Clean up temp file
    os.unlink(tmp_file_path)

    # Text Splitter
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, chunk_overlap=100)
    texts = text_splitter.split_documents(documents)

    # Embedding Logic - GPT ë˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œëŠ” OpenAI ì„ë² ë”© ì‚¬ìš©
    if provider in ["GPT-4o (ìƒìš©/ê³ í’ˆì§ˆ)", "í•˜ì´ë¸Œë¦¬ë“œ (GPTë²¡í„°ì¶”ë¡ +Ollamaë‹µë³€)"]:
        if not _api_key:
            st.error("OpenAI API Key Required")
            st.stop()
        embeddings_model = OpenAIEmbeddings(
            model="text-embedding-3-small", openai_api_key=_api_key)
        collection_name = "openai_collection"
    else:
        # HuggingFace Embeddings (ë¬´ë£Œ, ë¡œì»¬ ì‹¤í–‰)
        embeddings_model = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )
        collection_name = "hf_collection"

    # Chroma DB - Persistent Client
    safe_name = "".join([c for c in file.name if c.isalnum()])
    provider_prefix = "gpt" if provider != "Ollama (ì„¤ì¹˜í˜•/ë³´ì•ˆ)" else "ollama"
    persist_dir = os.path.join(
        os.getcwd(), ".chroma_db", provider_prefix, safe_name)

    client = chromadb.PersistentClient(path=persist_dir)

    db = Chroma.from_documents(
        texts,
        embeddings_model,
        client=client,
        collection_name=collection_name
    )

    return db.as_retriever(search_kwargs={"k": 5})


if uploaded_file is not None:
    try:
        retriever = embed_file(uploaded_file, model_provider, openai_key)
    except Exception as e:
        st.error(f"Error: {e}")
        st.stop()

    # Session State
    if "messages" not in st.session_state:
        st.session_state["messages"] = [
            {"role": "assistant", "content": "ë„¤, ë¬¸ì„œë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸í•´ ì£¼ì„¸ìš”."}]

    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).write(msg["content"])

    # Chat Input
    if prompt_message := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
        st.chat_message("user").write(prompt_message)
        st.session_state.messages.append(
            {"role": "user", "content": prompt_message})

        with st.chat_message("assistant"):
            status_container = st.empty()

            # Retrieval
            docs = retriever.invoke(prompt_message)
            context_text = "\n\n".join(doc.page_content for doc in docs)

            # Generation
            if model_provider == "GPT-4o (ìƒìš©/ê³ í’ˆì§ˆ)":
                llm = ChatOpenAI(
                    model="gpt-4o",
                    temperature=0,
                    openai_api_key=openai_key,
                    streaming=True,
                    callbacks=[StreamHandler(status_container)]
                )

                system_prompt = (
                    "ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ ë‹µë³€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
                    "ì•„ë˜ ì œê³µëœ ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ìƒì„¸í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n\n"
                    f"[ë¬¸ë§¥]\n{context_text}\n\n"
                    "[ì§€ì‹œì‚¬í•­]\n"
                    "- ë¬¸ë§¥ì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì•„ êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n"
                    "- í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n\n"
                    "[ì½˜í…ì¸  í•„í„°ë§ - í•„ìˆ˜]\n"
                    "- êµìœ¡ì ìœ¼ë¡œ ë¶€ì ì ˆí•œ í‘œí˜„(ë‚œë´‰, ë°”ëŒë‘¥ì´, ìƒ‰ê³¨ ë“±)ì€ ìˆœí™”ëœ í‘œí˜„ìœ¼ë¡œ ëŒ€ì²´í•˜ì„¸ìš”.\n"
                    "- ì„ ì •ì ì´ê±°ë‚˜ í­ë ¥ì ì¸ ë¬˜ì‚¬ëŠ” í”¼í•˜ê³  êµìœ¡ì ìœ¼ë¡œ ì í•©í•œ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.\n"
                    "- í•™ìƒì—ê²Œ ì í•©í•œ í’ˆìœ„ ìˆëŠ” ì–¸ì–´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
                )

                response = llm.invoke([
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=prompt_message)
                ])

            elif model_provider == "í•˜ì´ë¸Œë¦¬ë“œ (GPTë²¡í„°ì¶”ë¡ +Ollamaë‹µë³€)":
                # 1ë‹¨ê³„: GPT ë²¡í„° ì¶”ë¡  - ì§ˆë¬¸ì˜ ì˜ë¯¸ì  í™•ì¥ (PDF ë‚´ìš© ì—†ì´ ì§ˆë¬¸ë§Œ ì „ì†¡)
                status_container.markdown("ğŸ§  GPT ë²¡í„° ì¶”ë¡  ì¤‘... (ì§ˆë¬¸ë§Œ ì „ì†¡, PDF ë‚´ìš© ë³´í˜¸)")

                semantic_expansion = get_semantic_expansion_from_gpt(
                    prompt_message, openai_key
                )

                # 2ë‹¨ê³„: í™•ì¥ëœ ë²¡í„° ê²€ìƒ‰ - GPTì˜ ì¶”ë¡  ê²°ê³¼ë¥¼ í™œìš© (ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰)
                status_container.markdown("ğŸ” GPT ì¶”ë¡  ê¸°ë°˜ í–¥ìƒëœ ë²¡í„° ê²€ìƒ‰ ì¤‘...")

                enhanced_docs = enhanced_vector_search(
                    retriever, prompt_message, semantic_expansion, k=10
                )

                # 3ë‹¨ê³„: Map-Reduceë¡œ ë¶„í•  ì²˜ë¦¬ (í† í° ì œí•œ ìš°íšŒ)
                response_content = map_reduce_with_ollama(
                    docs=enhanced_docs,
                    question=prompt_message,
                    semantic_expansion=semantic_expansion,
                    ollama_url=ollama_url,
                    ollama_key=ollama_key,
                    status_container=status_container,
                    batch_size=2
                )

                # ì„¸ì…˜ì— ì €ì¥í•˜ê³  ì¢…ë£Œ
                st.session_state.messages.append(
                    {"role": "assistant", "content": response_content})
                st.stop()

            else:  # Ollama (ì„¤ì¹˜í˜•/ë³´ì•ˆ)
                headers = {}
                if ollama_key:
                    headers["Authorization"] = f"Bearer {ollama_key}"

                llm = ChatOllama(
                    base_url=ollama_url,
                    model="gemma3:27b",
                    temperature=0,
                    streaming=True,
                    callbacks=[StreamHandler(status_container)],
                    client_kwargs={"headers": headers} if headers else {}
                )

                system_prompt = (
                    "ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ ë‹µë³€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
                    "ì•„ë˜ ì œê³µëœ ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ìƒì„¸í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n\n"
                    f"[ë¬¸ë§¥]\n{context_text}\n\n"
                    "[ì§€ì‹œì‚¬í•­]\n"
                    "- ë¬¸ë§¥ì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì•„ êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n"
                    "- í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n\n"
                    "[ì½˜í…ì¸  í•„í„°ë§ - í•„ìˆ˜]\n"
                    "- êµìœ¡ì ìœ¼ë¡œ ë¶€ì ì ˆí•œ í‘œí˜„(ë‚œë´‰, ë°”ëŒë‘¥ì´, ìƒ‰ê³¨ ë“±)ì€ ìˆœí™”ëœ í‘œí˜„ìœ¼ë¡œ ëŒ€ì²´í•˜ì„¸ìš”.\n"
                    "- ì„ ì •ì ì´ê±°ë‚˜ í­ë ¥ì ì¸ ë¬˜ì‚¬ëŠ” í”¼í•˜ê³  êµìœ¡ì ìœ¼ë¡œ ì í•©í•œ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.\n"
                    "- í•™ìƒì—ê²Œ ì í•©í•œ í’ˆìœ„ ìˆëŠ” ì–¸ì–´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
                )

                response = llm.invoke([
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=prompt_message)
                ])

            response_content = response.content

        st.session_state.messages.append(
            {"role": "assistant", "content": response_content})
