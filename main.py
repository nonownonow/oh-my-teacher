# Streamlit Cloud ë°°í¬ìš© (Linux í™˜ê²½)
try:
    __import__('pysqlite3')
    import sys
    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
except ImportError:
    pass

from langchain_core.documents import Document
from langchain_core.messages import HumanMessage, SystemMessage
import pdfplumber
import os
import streamlit as st
import tempfile
import chromadb
from dotenv import load_dotenv
from langchain.callbacks.base import BaseCallbackHandler
from streamlit_extras.buy_me_a_coffee import button
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_ollama import ChatOllama
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter

# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ì½ê¸° (Ollamaë§Œ)
OLLAMA_API_KEY = os.getenv("OLLAMA_AI_API", "")

# ì œëª© ë° ìŠ¤íƒ€ì¼
st.set_page_config(page_title="ë‚˜ì˜ ê³¼ì™¸ ì„ ìƒë‹˜ ğŸ‘¨â€ğŸ«", page_icon="ğŸ‘¨â€ğŸ«")
st.title("ë‚˜ì˜ ê³¼ì™¸ ì„ ìƒë‹˜")
st.markdown("""
<style>
    .reportview-container {
        background: #f0f2f6
    }
</style>
""", unsafe_allow_html=True)
st.write("---")

# --------------------------------------------------------------------------------
# Sidebar
# --------------------------------------------------------------------------------
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    # Model Selection
    model_provider = st.radio(
        "ëª¨ë¸ ì„ íƒ",
        ["GPT-4o (ìƒìš©/ê³ í’ˆì§ˆ)", "Ollama (ì„¤ì¹˜í˜•/ë³´ì•ˆ)", "í•˜ì´ë¸Œë¦¬ë“œ (GPTë²¡í„°ì¶”ë¡ +Ollamaë‹µë³€)"],
        index=2,
        help="í•˜ì´ë¸Œë¦¬ë“œ: GPTê°€ ì§ˆë¬¸ë§Œ ë°›ì•„ ë²¡í„° ì¶”ë¡ (ì˜ë¯¸ í™•ì¥) ìˆ˜í–‰ â†’ Ollamaê°€ í–¥ìƒëœ ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ê³ í’ˆì§ˆ ë‹µë³€ ìƒì„± (PDF ë‚´ìš© ë³´í˜¸)"
    )

    ollama_url = "https://ollama.com"  # Ollama Cloud API
    ollama_key = OLLAMA_API_KEY

    # GPT/í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œì¼ ë•Œ OpenAI í‚¤ ì™¸ë¶€ ì…ë ¥ (í•­ìƒ)
    openai_key = ""
    if model_provider in ["GPT-4o (ìƒìš©/ê³ í’ˆì§ˆ)", "í•˜ì´ë¸Œë¦¬ë“œ (GPTë²¡í„°ì¶”ë¡ +Ollamaë‹µë³€)"]:
        openai_key = st.text_input('OpenAI API Key', type="password", help="GPT ëª¨ë¸ ì‚¬ìš©ì„ ìœ„í•œ API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”")

    st.divider()
    button(username="{ê³„ì • ID}", floating=False, width=221)

# File Upload
uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì˜¬ë ¤ì£¼ì„¸ìš”!", type=['pdf'])
st.write("---")

# --------------------------------------------------------------------------------
# Logic
# --------------------------------------------------------------------------------


class StreamHandler(BaseCallbackHandler):
    def __init__(self, container, initial_text=""):
        self.container = container
        self.text = initial_text

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text)


def get_gpt_reasoning_answer(question: str, api_key: str) -> str:
    """
    GPTê°€ ìì‹ ì˜ í•™ìŠµ ë°ì´í„°ë§Œìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€ (PDF ë‚´ìš© ì—†ì´)
    ê³ í’ˆì§ˆ ì¶”ë¡  ë‹µë³€ ìƒì„± - ìƒì„¸í•˜ê³  í¬ê´„ì ì¸ ë‹µë³€
    """
    llm = ChatOpenAI(
        model="gpt-4o",  # ê³ í’ˆì§ˆ ì¶”ë¡ 
        temperature=0.2,  # ì•½ê°„ì˜ ì°½ì˜ì„±ìœ¼ë¡œ ë” í’ë¶€í•œ ë‹µë³€
        max_tokens=2000,  # ì¶©ë¶„í•œ ë‹µë³€ ê¸¸ì´ í™•ë³´
        openai_api_key=api_key,
    )

    reasoning_prompt = f"""ë‹¹ì‹ ì€ í•´ë‹¹ ë¶„ì•¼ì˜ ìµœê³  ì „ë¬¸ê°€ì´ì ì—´ì •ì ì¸ ì„ ìƒë‹˜ì…ë‹ˆë‹¤.
í•™ìƒì´ ì•„ë˜ ì§ˆë¬¸ì„ í–ˆìŠµë‹ˆë‹¤. ë‹¹ì‹ ì˜ ë°©ëŒ€í•œ ì§€ì‹ì„ ì´ë™ì›í•˜ì—¬
ìµœëŒ€í•œ ìƒì„¸í•˜ê³  í¬ê´„ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

[í•™ìƒì˜ ì§ˆë¬¸]
{question}

[ë‹µë³€ ì‘ì„± ê·œì¹™ - ë°˜ë“œì‹œ ì¤€ìˆ˜]

1. **ë¶„ëŸ‰ ìš”êµ¬ì‚¬í•­**: ìµœì†Œ 500ì ì´ìƒ, ê°€ëŠ¥í•˜ë©´ 1000ì ì´ìƒìœ¼ë¡œ ìƒì„¸í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.

2. **êµ¬ì¡°ì  ë‹µë³€**: ë‹¤ìŒ êµ¬ì¡°ë¡œ ë‹µë³€í•˜ì„¸ìš”:
   - ğŸ“Œ í•µì‹¬ ë‹µë³€ (ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì  ë‹µë³€)
   - ğŸ“š ë°°ê²½ ì„¤ëª… (ë§¥ë½, ì—­ì‚¬ì /ë¬¸í™”ì  ë°°ê²½)
   - ğŸ‘¥ ê´€ë ¨ ì¸ë¬¼/ìš”ì†Œ (ë“±ì¥ì¸ë¬¼, í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª…)
   - ğŸ” ì‹¬ì¸µ ë¶„ì„ (ì£¼ì œì˜ ì˜ë¯¸, ìƒì§•, êµí›ˆ)
   - ğŸ’¡ ì¶”ê°€ ê´€ì  (ë‹¤ë¥¸ í•´ì„, ê´€ë ¨ ì§€ì‹)

3. **ìƒì„¸ ì„¤ëª… ì›ì¹™**:
   - ë‹¨ìˆœ ë‚˜ì—´ì´ ì•„ë‹Œ ê° í•­ëª©ì— ëŒ€í•œ ì¶©ë¶„í•œ ì„¤ëª… í¬í•¨
   - êµ¬ì²´ì ì¸ ì˜ˆì‹œì™€ ê·¼ê±° ì œì‹œ
   - ì „ë¬¸ ìš©ì–´ëŠ” ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…
   - ì¸ê³¼ê´€ê³„ì™€ ë…¼ë¦¬ì  íë¦„ ëª…í™•íˆ

4. **êµìœ¡ì  í’ˆì§ˆ**:
   - í•™ìƒì´ ê¹Šì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì¹œì ˆí•˜ê²Œ ì„¤ëª…
   - ì•”ê¸°ê°€ ì•„ë‹Œ ì´í•´ ì¤‘ì‹¬ì˜ ì„¤ëª…
   - ê´€ë ¨ ë°°ê²½ ì§€ì‹ë„ í•¨ê»˜ ì œê³µ

5. **í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê³  í’ë¶€í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.**

[ìƒì„¸í•œ ë‹µë³€]"""

    response = llm.invoke([HumanMessage(content=reasoning_prompt)])
    return response.content


def refine_answer_with_gpt(draft_answer: str, question: str, api_key: str) -> str:
    """
    Ollamaê°€ ìƒì„±í•œ ì´ˆì•ˆ ë‹µë³€ì„ GPTê°€ êµì •
    ë” ì •í™•í•˜ê³  ì •êµí•˜ê²Œ ë‹¤ë“¬ìŒ
    """
    llm = ChatOpenAI(
        model="gpt-4o",
        temperature=0,
        openai_api_key=api_key,
    )

    refine_prompt = f"""ë‹¹ì‹ ì€ ì „ë¬¸ í¸ì§‘ìì…ë‹ˆë‹¤.
ì•„ë˜ ì´ˆì•ˆ ë‹µë³€ì„ ê²€í† í•˜ê³  ë” ì •í™•í•˜ê³  ì •êµí•˜ê²Œ êµì •í•´ì£¼ì„¸ìš”.

[ì›ë³¸ ì§ˆë¬¸]
{question}

[ì´ˆì•ˆ ë‹µë³€]
{draft_answer}

[êµì • ì§€ì‹œì‚¬í•­]
1. ì‚¬ì‹¤ì  ì˜¤ë¥˜ê°€ ìˆìœ¼ë©´ ìˆ˜ì •í•˜ì„¸ìš”.
2. ë…¼ë¦¬ì  íë¦„ì„ ê°œì„ í•˜ì„¸ìš”.
3. ë¶ˆëª…í™•í•œ í‘œí˜„ì„ ëª…í™•í•˜ê²Œ ë‹¤ë“¬ìœ¼ì„¸ìš”.
4. ì¤‘ë³µëœ ë‚´ìš©ì€ ì •ë¦¬í•˜ì„¸ìš”.
5. êµìœ¡ì ìœ¼ë¡œ ë¶€ì ì ˆí•œ í‘œí˜„ì€ ìˆœí™”í•˜ì„¸ìš”.
6. í•µì‹¬ ë‚´ìš©ì€ ìœ ì§€í•˜ë©´ì„œ í’ˆì§ˆì„ ë†’ì´ì„¸ìš”.
7. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì„±í•˜ì„¸ìš”.

[êµì •ëœ ë‹µë³€]"""

    response = llm.invoke([HumanMessage(content=refine_prompt)])
    return response.content


def verify_with_ollama_pdf(
    gpt_answer: str,
    question: str,
    pdf_context: str,
    ollama_url: str,
    ollama_key: str,
    status_container
) -> str:
    """
    GPT ë‹µë³€ì„ PDF ì›ë³¸ ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ Ollamaê°€ ìµœì¢… ê²€ì¦
    PDFì— ì—†ëŠ” ê±°ì§“ ì •ë³´ëŠ” ì‚­ì œí•˜ê³ , PDF ê¸°ë°˜ ì§„ì‹¤ë§Œ ë‚¨ê¹€
    """
    headers = {}
    if ollama_key:
        headers["Authorization"] = f"Bearer {ollama_key}"

    llm = ChatOllama(
        base_url=ollama_url,
        model="gemma3:27b",
        temperature=0,
        streaming=True,
        callbacks=[StreamHandler(status_container)],
        client_kwargs={"headers": headers} if headers else {}
    )

    # Ollama í† í° í•œë„ ëŒ€ì‘: ë¬¸ë§¥ í¬ê¸° ì œí•œ
    limited_pdf_context = pdf_context[:30000] if len(pdf_context) > 30000 else pdf_context
    limited_gpt_answer = gpt_answer[:10000] if len(gpt_answer) > 10000 else gpt_answer

    verify_prompt = f"""ë‹¹ì‹ ì€ ì—„ê²©í•œ íŒ©íŠ¸ì²´ì»¤ì…ë‹ˆë‹¤.
GPTê°€ ì‘ì„±í•œ ë‹µë³€ì„ PDF ì›ë³¸ ë¬¸ì„œì™€ ëŒ€ì¡°í•˜ì—¬ ì² ì €íˆ ê²€ì¦í•˜ì„¸ìš”.

[PDF ì›ë³¸ ë¬¸ì„œ - ìœ ì¼í•œ ì§„ì‹¤ì˜ ê¸°ì¤€]
{limited_pdf_context}

[GPTê°€ ì‘ì„±í•œ ë‹µë³€ - ê²€ì¦ ëŒ€ìƒ]
{limited_gpt_answer}

[ê²€ì¦ ê·œì¹™ - ë°˜ë“œì‹œ ì¤€ìˆ˜]
1. PDF ë¬¸ì„œê°€ ìœ ì¼í•œ ì§„ì‹¤ì…ë‹ˆë‹¤. PDFì— ì—†ìœ¼ë©´ ê±°ì§“ì…ë‹ˆë‹¤.
2. ë“±ì¥ì¸ë¬¼: PDFì— ëª…ì‹œëœ ì´ë¦„ë§Œ ì‚¬ìš©í•˜ì„¸ìš”. PDFì— ì—†ëŠ” ì¸ë¬¼ì€ ì‚­ì œí•˜ì„¸ìš”.
3. ê´€ê³„: PDFì— ëª…ì‹œëœ ê´€ê³„ë§Œ ì‚¬ìš©í•˜ì„¸ìš”. ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
4. ì‚¬ê±´: PDFì— ìˆëŠ” ì‚¬ê±´ë§Œ í¬í•¨í•˜ì„¸ìš”.
5. GPTê°€ ì–¸ê¸‰í–ˆì§€ë§Œ PDFì— ì—†ëŠ” ëª¨ë“  ì •ë³´ëŠ” ê³¼ê°íˆ ì‚­ì œí•˜ì„¸ìš”.
6. í™•ì‹ ì´ ì—†ìœ¼ë©´ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

[ì½˜í…ì¸  í•„í„°ë§]
- êµìœ¡ì ìœ¼ë¡œ ë¶€ì ì ˆí•œ í‘œí˜„ì€ ìˆœí™”í•˜ì„¸ìš”.

[ìµœì¢… ë‹µë³€]"""

    response = llm.invoke([
        SystemMessage(content=verify_prompt),
        HumanMessage(content=question)
    ])

    return response.content


def get_semantic_expansion_from_gpt(question: str, api_key: str) -> dict:
    """
    GPTë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì˜ ì˜ë¯¸ì  í™•ì¥ì„ ìˆ˜í–‰ (ë²¡í„° ì¶”ë¡ )
    PDF ë‚´ìš© ì—†ì´ ì§ˆë¬¸ë§Œ ì „ì†¡í•˜ì—¬ ê´€ë ¨ ê°œë…, ë™ì˜ì–´, í•˜ìœ„ ì§ˆë¬¸ì„ ìƒì„±
    ì´ë¥¼ í†µí•´ ë²¡í„° ê²€ìƒ‰ì˜ í’ˆì§ˆì„ í–¥ìƒì‹œí‚´
    """
    llm = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0.3,  # ì•½ê°„ì˜ ì°½ì˜ì„± í—ˆìš©
        openai_api_key=api_key,
    )

    # GPTì—ê²Œ ì§ˆë¬¸ì˜ ì˜ë¯¸ì  í™•ì¥ ìš”ì²­ (ë²¡í„° ê³µê°„ì—ì„œì˜ ê´€ê³„ ì¶”ë¡ )
    expansion_prompt = f"""ë‹¹ì‹ ì€ ì˜ë¯¸ë¡ ì  ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ë²¡í„° ê²€ìƒ‰ í’ˆì§ˆì„ ë†’ì´ê¸° ìœ„í•œ ì˜ë¯¸ì  í™•ì¥ì„ ìˆ˜í–‰í•˜ì„¸ìš”.

[ì›ë³¸ ì§ˆë¬¸]
{question}

[ì§€ì‹œì‚¬í•­]
JSON í˜•ì‹ìœ¼ë¡œ ë‹¤ìŒì„ ì œê³µí•˜ì„¸ìš”:
1. "core_concepts": ì§ˆë¬¸ì˜ í•µì‹¬ ê°œë… í‚¤ì›Œë“œ (3-5ê°œ)
2. "synonyms": ê° í•µì‹¬ ê°œë…ì˜ ë™ì˜ì–´/ìœ ì‚¬ì–´ (ê°œë…ë‹¹ 2-3ê°œ)
3. "sub_questions": ì›ë³¸ ì§ˆë¬¸ì„ ë‹µí•˜ê¸° ìœ„í•´ í•„ìš”í•œ í•˜ìœ„ ì§ˆë¬¸ë“¤ (3-5ê°œ)
4. "related_topics": ê´€ë ¨ë  ìˆ˜ ìˆëŠ” ì£¼ì œ/ë§¥ë½ (3-5ê°œ)
5. "search_queries": ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•  ìµœì í™”ëœ ì¿¼ë¦¬ë¬¸ (3-5ê°œ)

[ì¶œë ¥ í˜•ì‹]
```json
{{
  "core_concepts": ["ê°œë…1", "ê°œë…2", ...],
  "synonyms": {{"ê°œë…1": ["ë™ì˜ì–´1", "ë™ì˜ì–´2"], ...}},
  "sub_questions": ["í•˜ìœ„ì§ˆë¬¸1", "í•˜ìœ„ì§ˆë¬¸2", ...],
  "related_topics": ["ì£¼ì œ1", "ì£¼ì œ2", ...],
  "search_queries": ["ì¿¼ë¦¬1", "ì¿¼ë¦¬2", ...]
}}
```"""

    response = llm.invoke([HumanMessage(content=expansion_prompt)])

    # JSON íŒŒì‹± ì‹œë„
    import json
    import re
    try:
        # JSON ë¸”ë¡ ì¶”ì¶œ
        json_match = re.search(r'```json\s*(.*?)\s*```', response.content, re.DOTALL)
        if json_match:
            return json.loads(json_match.group(1))
        else:
            # JSON ë¸”ë¡ ì—†ì´ ì§ì ‘ íŒŒì‹± ì‹œë„
            return json.loads(response.content)
    except json.JSONDecodeError:
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
        return {
            "core_concepts": [question],
            "synonyms": {},
            "sub_questions": [question],
            "related_topics": [],
            "search_queries": [question]
        }


def enhanced_vector_search(retriever, question: str, semantic_expansion: dict, k: int = 5) -> list:
    """
    GPTì˜ ì˜ë¯¸ì  í™•ì¥ì„ í™œìš©í•œ í–¥ìƒëœ ë²¡í„° ê²€ìƒ‰
    ì—¬ëŸ¬ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ í›„ ì¤‘ë³µ ì œê±° ë° ê²°ê³¼ ë³‘í•©
    """
    all_docs = []
    seen_contents = set()

    # 1. ì›ë³¸ ì§ˆë¬¸ìœ¼ë¡œ ê²€ìƒ‰
    original_docs = retriever.invoke(question)
    for doc in original_docs:
        if doc.page_content not in seen_contents:
            seen_contents.add(doc.page_content)
            all_docs.append(doc)

    # 2. í™•ì¥ëœ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ì¶”ê°€ ê²€ìƒ‰
    search_queries = semantic_expansion.get("search_queries", [])
    for query in search_queries[:3]:  # ìµœëŒ€ 3ê°œ ì¿¼ë¦¬
        try:
            docs = retriever.invoke(query)
            for doc in docs:
                if doc.page_content not in seen_contents:
                    seen_contents.add(doc.page_content)
                    all_docs.append(doc)
        except Exception:
            continue

    # 3. í•˜ìœ„ ì§ˆë¬¸ìœ¼ë¡œ ì¶”ê°€ ê²€ìƒ‰
    sub_questions = semantic_expansion.get("sub_questions", [])
    for sub_q in sub_questions[:2]:  # ìµœëŒ€ 2ê°œ í•˜ìœ„ ì§ˆë¬¸
        try:
            docs = retriever.invoke(sub_q)
            for doc in docs:
                if doc.page_content not in seen_contents:
                    seen_contents.add(doc.page_content)
                    all_docs.append(doc)
        except Exception:
            continue

    # ê²°ê³¼ ê°œìˆ˜ ì œí•œ
    return all_docs[:k]


def map_reduce_with_ollama(
    docs: list,
    question: str,
    semantic_expansion: dict,
    ollama_url: str,
    ollama_key: str,
    status_container,
    gpt_reasoning_answer: str = "",
    batch_size: int = 2
) -> str:
    """
    Map-Reduce íŒ¨í„´ìœ¼ë¡œ ë¬¸ì„œë¥¼ ë¶„í•  ì²˜ë¦¬ í›„ í•©ì¹¨
    1. Map: ê° ë¬¸ì„œ ë°°ì¹˜ì—ì„œ ê´€ë ¨ ì •ë³´ ì¶”ì¶œ
    2. Reduce: GPT ì¶”ë¡  ë‹µë³€(ë†’ì€ ê°€ì¤‘ì¹˜) + ì¶”ì¶œëœ ì •ë³´ë“¤ì„ í•©ì³ì„œ ìµœì¢… ë‹µë³€ ìƒì„±
    """
    headers = {}
    if ollama_key:
        headers["Authorization"] = f"Bearer {ollama_key}"

    # Map ë‹¨ê³„ìš© LLM (ìŠ¤íŠ¸ë¦¬ë° ì—†ì´)
    map_llm = ChatOllama(
        base_url=ollama_url,
        model="gemma3:27b",
        temperature=0,
        streaming=False,
        client_kwargs={"headers": headers} if headers else {}
    )

    # ë¬¸ì„œë¥¼ ë°°ì¹˜ë¡œ ë¶„í• 
    batches = [docs[i:i + batch_size] for i in range(0, len(docs), batch_size)]

    # Map ë‹¨ê³„: ê° ë°°ì¹˜ì—ì„œ ê´€ë ¨ ì •ë³´ ì¶”ì¶œ (ì¶œë ¥ ì—†ìŒ)
    extracted_infos = []
    for idx, batch in enumerate(batches):

        batch_content = "\n\n".join(doc.page_content for doc in batch)

        map_prompt = f"""ë‹¤ìŒ ë¬¸ì„œì—ì„œ ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸° ìœ„í•´ í•„ìš”í•œ í•µì‹¬ ì •ë³´ë§Œ ì¶”ì¶œí•˜ì„¸ìš”.

[ë¬¸ì„œ]
{batch_content}

[ì§ˆë¬¸]
{question}

[í•µì‹¬ ê°œë… ì°¸ê³ ]
{', '.join(semantic_expansion.get('core_concepts', []))}

[ì§€ì‹œì‚¬í•­]
- ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ë§Œ ê°„ê²°í•˜ê²Œ ì¶”ì¶œí•˜ì„¸ìš”.
- ë¶ˆí•„ìš”í•œ ì •ë³´ëŠ” ì œì™¸í•˜ì„¸ìš”.
- ê´€ë ¨ ì •ë³´ê°€ ì—†ìœ¼ë©´ "ê´€ë ¨ ì •ë³´ ì—†ìŒ"ì´ë¼ê³  ë‹µí•˜ì„¸ìš”.
- êµìœ¡ì ìœ¼ë¡œ ë¶€ì ì ˆí•œ í‘œí˜„ì€ ìˆœí™”ëœ í‘œí˜„ìœ¼ë¡œ ëŒ€ì²´í•˜ì„¸ìš”.

[ì¶”ì¶œëœ ì •ë³´]"""

        try:
            response = map_llm.invoke([HumanMessage(content=map_prompt)])
            if "ê´€ë ¨ ì •ë³´ ì—†ìŒ" not in response.content:
                extracted_infos.append(response.content)
        except Exception:
            continue

    if not extracted_infos:
        return "ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # Reduce ë‹¨ê³„: ì¶”ì¶œëœ ì •ë³´ë“¤ì„ í•©ì³ì„œ ìµœì¢… ë‹µë³€ ìƒì„±
    combined_info = "\n\n---\n\n".join(extracted_infos)

    # Reduceìš© LLM (ìŠ¤íŠ¸ë¦¬ë° ì—†ì´ - ì¤‘ê°„ ì¶œë ¥ ìˆ¨ê¹€)
    reduce_llm = ChatOllama(
        base_url=ollama_url,
        model="gemma3:27b",
        temperature=0,
        streaming=False,
        client_kwargs={"headers": headers} if headers else {}
    )

    expansion_info = f"""[GPT ë²¡í„° ì¶”ë¡  ê²°ê³¼]
- í•µì‹¬ ê°œë…: {', '.join(semantic_expansion.get('core_concepts', []))}
- ê´€ë ¨ ì£¼ì œ: {', '.join(semantic_expansion.get('related_topics', []))}
- ë¶„ì„ ê´€ì : {', '.join(semantic_expansion.get('sub_questions', [])[:3])}"""

    # GPT ì¶”ë¡  ë‹µë³€ ì„¹ì…˜ (ì°¸ê³ ìš©, ë‚®ì€ ê°€ì¤‘ì¹˜)
    gpt_section = ""
    if gpt_reasoning_answer:
        gpt_section = f"""
[GPT ì°¸ê³  ë‹µë³€ - êµ¬ì¡°/í‘œí˜„ë§Œ ì°¸ê³ ]
{gpt_reasoning_answer}

"""

    reduce_prompt = f"""ë‹¹ì‹ ì€ ì¹œì ˆí•œ ê³¼ì™¸ ì„ ìƒë‹˜ì…ë‹ˆë‹¤.
ì•„ë˜ ì •ë³´ë“¤ì„ ì¢…í•©í•˜ì—¬ í•™ìƒì˜ ì§ˆë¬¸ì— ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.

[â˜…â˜…â˜… PDF ë¬¸ì„œ ì •ë³´ - ê°€ì¤‘ì¹˜ ìµœìš°ì„  â˜…â˜…â˜…]
{combined_info}

{expansion_info}
{gpt_section}
[ê°€ì¤‘ì¹˜ ì ìš© ê·œì¹™ - ë§¤ìš° ì¤‘ìš”]
1. PDF ë¬¸ì„œ ì •ë³´ì— ê°€ì¥ ë†’ì€ ê°€ì¤‘ì¹˜(70%)ë¥¼ ë¶€ì—¬í•˜ì„¸ìš”.
2. GPT ë‹µë³€ì€ êµ¬ì¡°ì™€ í‘œí˜„ ì°¸ê³ ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ì„¸ìš”(30%).
3. PDF ë¬¸ì„œì™€ GPT ë‹µë³€ì´ ì¶©ëŒí•  ê²½ìš°, PDF ë¬¸ì„œë¥¼ ìš°ì„ í•˜ì„¸ìš”.
4. ë“±ì¥ì¸ë¬¼ ì´ë¦„, ê´€ê³„, ì‚¬ê±´ì€ ë°˜ë“œì‹œ PDF ë¬¸ì„œ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.
5. GPTê°€ ì–¸ê¸‰í–ˆì§€ë§Œ PDFì— ì—†ëŠ” ì •ë³´ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

[ì§€ì‹œì‚¬í•­]
- PDF ë¬¸ì„œì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ êµ¬ì„±í•˜ì„¸ìš”.
- GPT ë‹µë³€ì˜ ì¢‹ì€ êµ¬ì¡°ì™€ í‘œí˜„ë§Œ ì°¸ê³ í•˜ì„¸ìš”.
- í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

[ì½˜í…ì¸  í•„í„°ë§ - í•„ìˆ˜]
- êµìœ¡ì ìœ¼ë¡œ ë¶€ì ì ˆí•œ í‘œí˜„(ë‚œë´‰, ë°”ëŒë‘¥ì´, ìƒ‰ê³¨ ë“±)ì€ ìˆœí™”ëœ í‘œí˜„ìœ¼ë¡œ ëŒ€ì²´í•˜ì„¸ìš”.
- ì„ ì •ì ì´ê±°ë‚˜ í­ë ¥ì ì¸ ë¬˜ì‚¬ëŠ” í”¼í•˜ê³  êµìœ¡ì ìœ¼ë¡œ ì í•©í•œ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.
- í•™ìƒì—ê²Œ ì í•©í•œ í’ˆìœ„ ìˆëŠ” ì–¸ì–´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."""

    response = reduce_llm.invoke([
        SystemMessage(content=reduce_prompt),
        HumanMessage(content=question)
    ])

    return response.content


@st.cache_resource(show_spinner="ë¬¸ì„œ ë¶„ì„ ë° ì„ë² ë”© ì¤‘...")
def embed_file(file, provider, _api_key):
    file_content = file.read()

    # Use a temporary directory for file storage to avoid clutter
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(file_content)
        tmp_file_path = tmp_file.name

    documents = []
    # PDF Parsing with pdfplumber (Text only)
    with pdfplumber.open(tmp_file_path) as pdf:
        for i, page in enumerate(pdf.pages):
            text = page.extract_text() or ""
            documents.append(Document(page_content=text, metadata={
                             "page": i+1, "source": file.name}))

    # Clean up temp file
    os.unlink(tmp_file_path)

    # Text Splitter
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, chunk_overlap=100)
    texts = text_splitter.split_documents(documents)

    # Embedding Logic - GPT ë˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œëŠ” OpenAI ì„ë² ë”© ì‚¬ìš©
    if provider in ["GPT-4o (ìƒìš©/ê³ í’ˆì§ˆ)", "í•˜ì´ë¸Œë¦¬ë“œ (GPTë²¡í„°ì¶”ë¡ +Ollamaë‹µë³€)"]:
        if not _api_key:
            st.error("OpenAI API Key Required")
            st.stop()
        embeddings_model = OpenAIEmbeddings(
            model="text-embedding-3-small", openai_api_key=_api_key)
        collection_name = "openai_collection"
    else:
        # HuggingFace Embeddings (ë¬´ë£Œ, ë¡œì»¬ ì‹¤í–‰)
        embeddings_model = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )
        collection_name = "hf_collection"

    # Chroma DB - Persistent Client
    safe_name = "".join([c for c in file.name if c.isalnum()])
    provider_prefix = "gpt" if provider != "Ollama (ì„¤ì¹˜í˜•/ë³´ì•ˆ)" else "ollama"
    persist_dir = os.path.join(
        os.getcwd(), ".chroma_db", provider_prefix, safe_name)

    client = chromadb.PersistentClient(path=persist_dir)

    db = Chroma.from_documents(
        texts,
        embeddings_model,
        client=client,
        collection_name=collection_name
    )

    retriever = db.as_retriever(search_kwargs={"k": 10})

    # ëª¨ë“  ì²­í¬ í…ìŠ¤íŠ¸ ë°˜í™˜ (GPT ì „ì²´ ë¬¸ë§¥ìš©)
    all_chunks_text = "\n\n---\n\n".join([doc.page_content for doc in texts])

    return retriever, all_chunks_text, len(texts)


if uploaded_file is not None:
    try:
        retriever, all_chunks_text, chunk_count = embed_file(uploaded_file, model_provider, openai_key)
        st.sidebar.info(f"ğŸ“„ ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ: {chunk_count}ê°œ ì²­í¬")
    except Exception as e:
        st.error(f"Error: {e}")
        st.stop()

    # Session State
    if "messages" not in st.session_state:
        st.session_state["messages"] = [
            {"role": "assistant", "content": "ë„¤, ë¬¸ì„œë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸í•´ ì£¼ì„¸ìš”."}]

    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).write(msg["content"])

    # Chat Input
    if prompt_message := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
        st.chat_message("user").write(prompt_message)
        st.session_state.messages.append(
            {"role": "user", "content": prompt_message})

        with st.chat_message("assistant"):
            status_container = st.empty()

            # Retrieval
            docs = retriever.invoke(prompt_message)
            context_text = "\n\n".join(doc.page_content for doc in docs)

            # Generation
            if model_provider == "GPT-4o (ìƒìš©/ê³ í’ˆì§ˆ)":
                # GPT-4oëŠ” ì „ì²´ PDF ë¬¸ë§¥ì„ ì‚¬ìš© (128K ì»¨í…ìŠ¤íŠ¸ í™œìš©)
                llm = ChatOpenAI(
                    model="gpt-4o",
                    temperature=0,
                    max_tokens=4000,  # ì¶©ë¶„í•œ ì‘ë‹µ ê¸¸ì´
                    openai_api_key=openai_key,
                    streaming=True,
                    callbacks=[StreamHandler(status_container)]
                )

                system_prompt = f"""ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ ë‹µë³€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ ì œê³µëœ **ì „ì²´ ë¬¸ì„œ ë‚´ìš©**ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ìƒì„¸í•˜ê³  í¬ê´„ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

[ì „ì²´ ë¬¸ì„œ ë‚´ìš©]
{all_chunks_text}

[ë‹µë³€ ì§€ì‹œì‚¬í•­]
1. **ì™„ì „ì„±**: ë¬¸ì„œ ì „ì²´ë¥¼ ê²€í† í•˜ì—¬ ê´€ë ¨ëœ ëª¨ë“  ì •ë³´ë¥¼ í¬í•¨í•˜ì„¸ìš”.
2. **êµ¬ì²´ì„±**: ë“±ì¥ì¸ë¬¼, ì‚¬ê±´, ê´€ê³„ ë“±ì„ ë¬¸ì„œì— ìˆëŠ” ê·¸ëŒ€ë¡œ ì •í™•íˆ ë‹µë³€í•˜ì„¸ìš”.
3. **êµ¬ì¡°í™”**: ì—¬ëŸ¬ í•­ëª©ì´ ìˆëŠ” ê²½ìš° ëª©ë¡ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”.
4. **ìƒì„¸ ì„¤ëª…**: ê° í•­ëª©ì— ëŒ€í•´ ì¶©ë¶„í•œ ì„¤ëª…ì„ ì œê³µí•˜ì„¸ìš”.
5. **í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.**

[ì½˜í…ì¸  í•„í„°ë§ - í•„ìˆ˜]
- êµìœ¡ì ìœ¼ë¡œ ë¶€ì ì ˆí•œ í‘œí˜„ì€ ìˆœí™”ëœ í‘œí˜„ìœ¼ë¡œ ëŒ€ì²´í•˜ì„¸ìš”.
- í•™ìƒì—ê²Œ ì í•©í•œ í’ˆìœ„ ìˆëŠ” ì–¸ì–´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."""

                response = llm.invoke([
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=prompt_message)
                ])

            elif model_provider == "í•˜ì´ë¸Œë¦¬ë“œ (GPTë²¡í„°ì¶”ë¡ +Ollamaë‹µë³€)":
                # ì§„í–‰ ìƒíƒœ í‘œì‹œìš©
                progress = status_container.empty()

                # 1ë‹¨ê³„: GPT ì¶”ë¡ 
                progress.markdown("ğŸ§  **1/4** GPT ì¶”ë¡  ì¤‘...")
                gpt_reasoning_answer = get_gpt_reasoning_answer(
                    prompt_message, openai_key
                )
                semantic_expansion = get_semantic_expansion_from_gpt(
                    prompt_message, openai_key
                )

                # 2ë‹¨ê³„: ë²¡í„° ê²€ìƒ‰
                progress.markdown("ğŸ” **2/4** ë²¡í„° ê²€ìƒ‰ ì¤‘...")
                enhanced_docs = enhanced_vector_search(
                    retriever, prompt_message, semantic_expansion, k=15  # ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰
                )

                # 3ë‹¨ê³„: Map-Reduce (ì¶œë ¥ ì—†ìŒ)
                progress.markdown("âš™ï¸ **3/4** ë¬¸ì„œ ë¶„ì„ ì¤‘...")
                draft_answer = map_reduce_with_ollama(
                    docs=enhanced_docs,
                    question=prompt_message,
                    semantic_expansion=semantic_expansion,
                    ollama_url=ollama_url,
                    ollama_key=ollama_key,
                    status_container=st.empty(),  # ë¹ˆ ì»¨í…Œì´ë„ˆ (ì¶œë ¥ ì•ˆ í•¨)
                    gpt_reasoning_answer=gpt_reasoning_answer,
                    batch_size=3
                )

                # 4ë‹¨ê³„: Ollama ìµœì¢… ê²€ì¦ (í™”ë©´ì— ì¶œë ¥)
                progress.markdown("âœ… **4/4** ìµœì¢… ê²€ì¦ ì¤‘...")
                import time
                time.sleep(0.5)  # ì§„í–‰ ìƒíƒœ í‘œì‹œë¥¼ ìœ„í•œ ì§§ì€ ëŒ€ê¸°
                progress.empty()  # ì§„í–‰ ìƒíƒœ ì œê±°

                verify_context = "\n\n".join(doc.page_content for doc in enhanced_docs[:10])  # ë” ë„“ì€ ê²€ì¦ ë¬¸ë§¥

                final_answer = verify_with_ollama_pdf(
                    gpt_answer=draft_answer,
                    question=prompt_message,
                    pdf_context=verify_context,
                    ollama_url=ollama_url,
                    ollama_key=ollama_key,
                    status_container=status_container
                )

                # ì„¸ì…˜ì— ì €ì¥í•˜ê³  ì¢…ë£Œ
                st.session_state.messages.append(
                    {"role": "assistant", "content": final_answer})
                st.stop()

            else:  # Ollama (ì„¤ì¹˜í˜•/ë³´ì•ˆ)
                headers = {}
                if ollama_key:
                    headers["Authorization"] = f"Bearer {ollama_key}"

                llm = ChatOllama(
                    base_url=ollama_url,
                    model="gemma3:27b",
                    temperature=0,
                    streaming=True,
                    callbacks=[StreamHandler(status_container)],
                    client_kwargs={"headers": headers} if headers else {}
                )

                # Ollama Cloud í† í° í•œë„ ëŒ€ì‘: ë§¤ìš° ì‘ì€ ë¬¸ë§¥ (í…ŒìŠ¤íŠ¸ìš©)
                MAX_CONTEXT_CHARS = 2000  # ì•„ì£¼ ì‘ê²Œ ì„¤ì •
                limited_context = context_text[:MAX_CONTEXT_CHARS] if len(context_text) > MAX_CONTEXT_CHARS else context_text

                system_prompt = f"""ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ ë‹µë³€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[ë¬¸ë§¥]
{limited_context}

[ì§€ì‹œì‚¬í•­]
- ë¬¸ë§¥ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
- í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”."""

                # ë””ë²„ê·¸: ì „ì²´ í”„ë¡¬í”„íŠ¸ ê¸¸ì´ í‘œì‹œ
                total_chars = len(system_prompt) + len(prompt_message)
                st.sidebar.write(f"ğŸ“Š ë¬¸ë§¥: {len(limited_context):,}ì")
                st.sidebar.write(f"ğŸ“Š ì „ì²´ í”„ë¡¬í”„íŠ¸: {total_chars:,}ì")

                response = llm.invoke([
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=prompt_message)
                ])

            response_content = response.content

        st.session_state.messages.append(
            {"role": "assistant", "content": response_content})
